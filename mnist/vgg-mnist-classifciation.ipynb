{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Udacity - Introduction to Machine Learning Course\n",
        "## Submitted by Abhijith Ganesh\n",
        "\n",
        "<hr/>\n",
        "\n",
        "### Recommended approach\n",
        "Using a Convoluted Neural Network and TL based on VGG19 (Weights of Imagenet)\n",
        "\n",
        "Use torchvision to classify hand written characters and identify them! Classic MNIST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1684863173282
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "from torchvision.datasets import MNIST\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision import transforms\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision.models import  VGG19_Weights, vgg19\n",
        "from datetime import datetime\n",
        "from torch.optim import Adam\n",
        "from torch import nn, save, no_grad, max\n",
        "import random\n",
        "\n",
        "batch_size = 16\n",
        "learning_rate = 10**(-3)\n",
        "num_epochs = 5"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setting Constants for the experiment\n",
        "\n",
        "<hr/>\n",
        "\n",
        "Setting values like batch_size, number of epochs and learning rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1684863173602
        }
      },
      "outputs": [],
      "source": [
        "def set_constants(batch_size: int, learning_rate: float, number_of_epochs: int) -> tuple:\n",
        "    batch_size = batch_size\n",
        "    learning_rate = learning_rate\n",
        "    no_of_epochs = number_of_epochs\n",
        "\n",
        "    return (batch_size, learning_rate, no_of_epochs)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Transformers and Loaders\n",
        "<hr/>\n",
        "\n",
        "### Addition of data and loaders\n",
        "\n",
        "\n",
        "Change the download boolean to download. You can set the path according to your setup!\n",
        "\n",
        "```python\n",
        "image_dataset = MNIST(root=f'{YOUR_PATH_NAME}', train=True, download=True)\n",
        "```\n",
        "\n",
        "### Transformers:\n",
        "I've resized it to 224x224 as VGG19 uses this dimension\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "gather": {
          "logged": 1684863174335
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f3fe707fb80>"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "im_show_transformer = transforms.Compose([\n",
        "    transforms.Grayscale(num_output_channels=1),    \n",
        "    transforms.ToTensor(),\n",
        "    transforms.Resize([224, 224]),\n",
        "    transforms.Normalize(mean=[0.5], std=[0.5]) \n",
        "])\n",
        "image_dataset = MNIST(root='./data', train=True, download=True, transform=im_show_transformer)\n",
        "split = 0.8\n",
        "\n",
        "train_len = int(len(image_dataset)*split*0.5)\n",
        "test_len  = len(image_dataset) - 2*train_len\n",
        "\n",
        "train_set, valid_set, test_set = random_split(image_dataset, [train_len, train_len, test_len])\n",
        "\n",
        "plt.imshow(train_set.dataset.data[random.randrange(1, len(train_set))], cmap='gray')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "gather": {
          "logged": 1684863174703
        }
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=True)\n",
        "valid_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Using VGG 19"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "gather": {
          "logged": 1684863175589
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ],
      "source": [
        "model = vgg19(weights=VGG19_Weights)\n",
        "in_features = model.classifier[6].in_features\n",
        "model.features[0] = nn.Conv2d(\n",
        "    1, 64, kernel_size = 3, stride=1, padding=1\n",
        ")\n",
        "model.classifier[6] = nn.Linear(\n",
        "    in_features,\n",
        "    10\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "gather": {
          "logged": 1684863175963
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "VGG(\n",
              "  (features): Sequential(\n",
              "    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (3): ReLU(inplace=True)\n",
              "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (6): ReLU(inplace=True)\n",
              "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (8): ReLU(inplace=True)\n",
              "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (11): ReLU(inplace=True)\n",
              "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (13): ReLU(inplace=True)\n",
              "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (15): ReLU(inplace=True)\n",
              "    (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (17): ReLU(inplace=True)\n",
              "    (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (20): ReLU(inplace=True)\n",
              "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (22): ReLU(inplace=True)\n",
              "    (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (24): ReLU(inplace=True)\n",
              "    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (26): ReLU(inplace=True)\n",
              "    (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (29): ReLU(inplace=True)\n",
              "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (31): ReLU(inplace=True)\n",
              "    (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (33): ReLU(inplace=True)\n",
              "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (35): ReLU(inplace=True)\n",
              "    (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
              "  (classifier): Sequential(\n",
              "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Dropout(p=0.5, inplace=False)\n",
              "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "    (4): ReLU(inplace=True)\n",
              "    (5): Dropout(p=0.5, inplace=False)\n",
              "    (6): Linear(in_features=4096, out_features=10, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "gather": {
          "logged": 1684863176283
        }
      },
      "outputs": [],
      "source": [
        "def set_loss_function():\n",
        "    return nn.CrossEntropyLoss()\n",
        "\n",
        "def set_optimizer(params, learning_rate):\n",
        "    return Adam(params, lr = learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "gather": {
          "logged": 1684863176713
        }
      },
      "outputs": [],
      "source": [
        "optimizer = set_optimizer(model.parameters(), learning_rate)\n",
        "criterion = set_loss_function()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Defining the validation function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "gather": {
          "logged": 1684863177067
        }
      },
      "outputs": [],
      "source": [
        "def validate_model(model, validation_loader) -> int:\n",
        "    model.eval()\n",
        "\n",
        "    total: int = 0\n",
        "    correct: int = 0\n",
        "\n",
        "    with no_grad():  # Disable gradient calculation during validation\n",
        "        for images, labels in validation_loader:\n",
        "            \n",
        "            outputs = model(images)\n",
        "            _, predicted = max(outputs.data, 1)\n",
        "\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "            print(correct)\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    return accuracy"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting with the training process 2023-05-23 17:33:22.304562\n"
          ]
        }
      ],
      "source": [
        "total_loss = 0\n",
        "\n",
        "for epochs in range(num_epochs):\n",
        "    iterations = 0\n",
        "    model.train()\n",
        "    \n",
        "    print(f\"Starting with the training process {datetime.now()}\")\n",
        "\n",
        "    for images, labels in train_loader:     \n",
        "        iterations += 1\n",
        "\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "       \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "       \n",
        "        optimizer.zero_grad()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    print(f\"Epoch [{epochs+1}/{num_epochs}], Average Loss: {avg_loss} at time {datetime.now()}\")\n",
        "\n",
        "    with no_grad():\n",
        "        accuracy = validate_model(model, valid_loader)\n",
        "        print(f\"At Epoch [{epochs+1}/{num_epochs}], Accuracy is {accuracy}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Add fields to store important values\n",
        "\n",
        "\n",
        "As a ML engineer, I'd prefer storing the data of the following :\n",
        "- Model (Obviously)\n",
        "- Optimizer\n",
        "- Batch Size\n",
        "- Loss Criterion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_custom_dict(model, optimizer, loss_criterion, batch_size = batch_size) -> dict:\n",
        "    custom_dictionary = {\n",
        "        \"model\": model.state_dict(),\n",
        "        \"optimizer\": optimizer.state_dict(),\n",
        "        \"loss\": loss_criterion,\n",
        "        \"hyperparameters\": {\n",
        "            \"batch_size\":batch_size\n",
        "        }\n",
        "    }\n",
        "    return custom_dictionary\n",
        "\n",
        "custom_dict = generate_custom_dict(model, optimizer, criterion)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Saving the file to VGG binary\n",
        "\n",
        "<hr/>\n",
        "\n",
        "```python\n",
        "def save_model(path:str):\n",
        "    torch.save(model.state_dict(), path)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def save_model(custom_dict:dict, path:str):\n",
        "    save(custom_dict, path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "save_model(custom_dict, f'vgg-19-{datetime.now().strftime(f\"%d_%m_%y_____%H_%M_%S\")}.pth')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.8 - Pytorch and Tensorflow",
      "language": "python",
      "name": "python38-azureml-pt-tf"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
